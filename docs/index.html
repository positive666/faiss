<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Welcome to Faiss Documentation &mdash; Faiss  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/tabs.css" type="text/css" />
    <link rel="canonical" href="https://faiss.ai/index.html" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Class list" href="cpp_api/classlist.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="#" class="icon icon-home"> Faiss
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Docs</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Home</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/facebookresearch/faiss/wiki">Wiki</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_api/classlist.html">Class list</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_api/filelist.html">File list</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_api/namespacelist.html">Namespace list</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_api/structlist.html">Struct list</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Faiss</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
      <li>Welcome to Faiss Documentation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="welcome-to-faiss-documentation">
<h1>Welcome to Faiss Documentation<a class="headerlink" href="#welcome-to-faiss-documentation" title="Permalink to this headline"></a></h1>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<a class="reference external image-reference" href="https://anaconda.org/pytorch/faiss-cpu"><img alt="https://img.shields.io/conda/vn/pytorch/faiss-cpu?label=conda" src="https://img.shields.io/conda/vn/pytorch/faiss-cpu?label=conda" /></a>
<img alt="https://img.shields.io/conda/pn/pytorch/faiss-cpu" src="https://img.shields.io/conda/pn/pytorch/faiss-cpu" />
<a class="reference external image-reference" href="https://github.com/facebookresearch/faiss/blob/master/LICENSE"><img alt="https://img.shields.io/github/license/facebookresearch/faiss" src="https://img.shields.io/github/license/facebookresearch/faiss" /></a>
<a class="reference external image-reference" href="https://circleci.com/gh/facebookresearch/faiss/tree/master"><img alt="https://circleci.com/gh/facebookresearch/faiss/tree/master.svg?style=shield" src="https://circleci.com/gh/facebookresearch/faiss/tree/master.svg?style=shield" /></a>
<a class="reference external image-reference" href="https://github.com/facebookresearch/faiss"><img alt="https://img.shields.io/github/stars/facebookresearch/faiss?style=social" src="https://img.shields.io/github/stars/facebookresearch/faiss?style=social" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
<section id="faiss">
<h1>Faiss<a class="headerlink" href="#faiss" title="Permalink to this headline"></a></h1>
<p>Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.</p>
<p>Faiss is written in C++ with complete wrappers for Python (versions 2 and 3). Some of the most useful algorithms are implemented on the GPU. It is developed by <a class="reference external" href="https://research.fb.com/category/facebook-ai-research-fair/">Facebook AI Research</a>.</p>
<section id="what-is-similarity-search">
<h2>What is similarity search?<a class="headerlink" href="#what-is-similarity-search" title="Permalink to this headline"></a></h2>
<p>Given a set of vectors <span class="math notranslate nohighlight">\(x_i\)</span> in dimension <span class="math notranslate nohighlight">\(d\)</span>, Faiss build a data structure in RAM from it.
After the structure is constructed, when given a new vector <span class="math notranslate nohighlight">\(x\)</span> in dimension <span class="math notranslate nohighlight">\(d\)</span> it performs efficiently the operation:</p>
<div class="math notranslate nohighlight">
\[j = argmin_i \lVert x - x_i \rVert\]</div>
<p>where <span class="math notranslate nohighlight">\(\lVert\cdot\rVert\)</span> is the Euclidean distance (<span class="math notranslate nohighlight">\(L^2\)</span>).</p>
<p>In Faiss terms, the data structure is an <em>index</em>, an object that has an <em>add</em> method to add <span class="math notranslate nohighlight">\(x_i\)</span> vectors.
Note that the <span class="math notranslate nohighlight">\(x_i\)</span>’s are assumed to be fixed.</p>
<p>Computing the argmin is the <em>search</em> operation on the index.</p>
<p>This is all what Faiss is about. It can also:</p>
<ul class="simple">
<li><p>return not just the nearest neighbor, but also the 2nd nearest, 3rd, …, k-th nearest neighbor</p></li>
<li><p>search several vectors at a time rather than one (batch processing). For many index types, this is faster than searching one vector after another</p></li>
<li><p>trade precision for speed, ie. give an incorrect result 10% of the time with a method that’s 10x faster or uses 10x less memory</p></li>
<li><p>perform maximum inner product search <span class="math notranslate nohighlight">\(argmax_i \langle x, x_i \rangle\)</span> instead of minimum Euclidean search. There is also limited support for other distances (L1, Linf, etc.).</p></li>
<li><p>return all elements that are within a given radius of the query point (range search)</p></li>
<li><p>store the index on disk rather than in RAM.</p></li>
</ul>
</section>
<section id="install">
<h2>Install<a class="headerlink" href="#install" title="Permalink to this headline"></a></h2>
<p>The recommended way to install Faiss is through <a class="reference external" href="https://conda.io">Conda</a>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ conda install -c pytorch faiss-cpu
</pre></div>
</div>
<p>The <code class="code docutils literal notranslate"><span class="pre">faiss-gpu</span></code> package provides CUDA-enabled indices:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ conda install -c pytorch faiss-gpu
</pre></div>
</div>
<p>Note that either package should be installed, but not both, as the latter is a superset of the former.</p>
</section>
<section id="research-foundations-of-faiss">
<h2>Research foundations of Faiss<a class="headerlink" href="#research-foundations-of-faiss" title="Permalink to this headline"></a></h2>
<p>Faiss is based on years of research. Most notably it implements:</p>
<ul class="simple">
<li><p>The inverted file from <a class="reference external" href="http://ieeexplore.ieee.org/abstract/document/1238663/">“Video google: A text retrieval approach to object matching in videos.”</a>, Sivic &amp; Zisserman, ICCV 2003. This is the key to non-exhaustive search in large datasets. Otherwise all searches would need to scan all elements in the index, which is prohibitive even if the operation to apply for each element is fast</p></li>
<li><p>The product quantization (PQ) method from <a class="reference external" href="https://hal.inria.fr/inria-00514462v2/document">“Product quantization for nearest neighbor search”</a>, Jégou &amp; al., PAMI 2011. This can be seen as a lossy compression technique for high-dimensional vectors, that allows relatively accurate reconstructions and distance computations in the compressed domain.</p></li>
<li><p>The three-level quantization (IVFADC-R aka <cite>IndexIVFPQR</cite>) method from <a class="reference external" href="https://arxiv.org/pdf/1102.3828">“Searching in one billion vectors: re-rank with source coding”</a>, Tavenard &amp; al., ICASSP’11.</p></li>
<li><p>The inverted multi-index from <a class="reference external" href="http://ieeexplore.ieee.org/abstract/document/6248038/">“The inverted multi-index”</a>, Babenko &amp; Lempitsky, CVPR 2012. This method greatly improves the speed of inverted indexing for fast/less accurate operating points.</p></li>
<li><p>The optimized PQ from <a class="reference external" href="http://ieeexplore.ieee.org/abstract/document/6678503/">“Optimized product quantization”</a>, He &amp; al, CVPR 2013. This method can be seen as a linear transformation of the vector space to make it more amenable for indexing with a product quantizer.</p></li>
<li><p>The pre-filtering of product quantizer distances from <a class="reference external" href="http://link.springer.com/chapter/10.1007/978-3-319-46475-6_48">“Polysemous codes”</a>, Douze &amp; al., ECCV 2016. This technique performs a binary filtering stage before computing PQ distances.</p></li>
<li><p>The GPU implementation and fast k-selection is described in <a class="reference external" href="https://arxiv.org/abs/1702.08734">“Billion-scale similarity search with GPUs”</a>, Johnson &amp; al, ArXiv 1702.08734, 2017</p></li>
<li><p>The HNSW indexing method from <a class="reference external" href="https://arxiv.org/abs/1603.09320">“Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs”</a>, Malkov &amp; al., ArXiv 1603.09320, 2016</p></li>
</ul>
<p>A general paper about product quantization and related methods: <a class="reference external" href="https://www.jstage.jst.go.jp/article/mta/6/1/6_2/_pdf">“A Survey of Product Quantization”</a>, Yusuke Matsui, Yusuke Uchida, Hervé Jégou,
Shin’ichi Satoh, ITE transactions on MTA, 2018.</p>
<p>The overview image below is from that paper (click on the image to enlarge it):</p>
<a class="reference external image-reference" href="https://raw.githubusercontent.com/wiki/facebookresearch/faiss/PQ_variants_Faiss_annotated.png"><img alt="PQ variants" src="https://raw.githubusercontent.com/wiki/facebookresearch/faiss/PQ_variants_Faiss_annotated.png" /></a>
<p>Image credit: <a class="reference external" href="http://yusukematsui.me">Yusuke Matsui</a>, thanks for allowing us to use it!</p>
<p>Methods that are implemented in Faiss are highlighted in red.</p>
<details><summary>Key to all references</summary>
<pre>
André+, “Cache Locality is not Enough: High-performance Nearest Neighbor Search with Product Quantization Fast Scan”, VLDB 15
André+, “Accelerated Nearest Neighbor Search with Quick ADC”, ICMR 17
André+, “Quicker ADC : Unlocking the Hidden Potential of Product Quantization with SIMD”, IEEE TPAMI 20
Babenko and Lempitsky, “The Inverted Multi-index”, CVPR 12
Babenko and Lempitsky, “Additive Quantization for Extreme Vector Compression”, CVPR 14
Babenko and Lempitsky, “The Inverted Multi-index”, IEEE TPAMI 15
Babenko and Lempitsky, “Tree Quantization for Large-scale Similarity Search and Classification”, CVPR 15
Babenko and Lempitsky, “Efficient Indexing of Billion-scale Datasets of Deep Descriptors”, CVPR 16
Babenko and Lempitsky, “Product Split Trees”, CVPR 17
Bagherinezhad+, “LCNN: Lookup-based Convolutional Neural Network”, CVPR 17
Baranchuk+, “Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors”, ECCV 18
Blalock and Guttag, “Bolt: Accelerated Data Mining with Fast Vector Compression”, KDD 17
Eghbali and Tahvildari, “Deep Spherical Quantization for Image Search”, CVPR 19
Douze+, “Polysemous Codes”, ECCV 16
Douze+, “Link and code: Fast Indexing with Graphs and Compact Regression Codes”, CVPR 18
Ge+, “Optimized Product Quantization”, IEEE TPAMI 14
Ge+, “Product Sparse Coding”, CVPR 14
He+, “K-means Hashing: An Affinity-preserving Quantization Method for Learning Binary Compact Codes”, CVPR 13
Heo+, “Short-list Selection with Residual-aware Distance Estimator for K-nearest Neighbor Search”, CVPR 16
Heo+, “Distance Encoded Product Quantization”, CVPR 14
Iwamura+, “What is the Most Efficient Way to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?”, ICCV 13
Jain+, “Approximate Search with Quantized Sparse Representations”, ECCV 16
Jégou+, “Product Quantization for Nearest Neighbor Search”, IEEE TPAMI 11
Jégou+, “Aggregating Local Descriptors into a Compact Image Representation”, CVPR 10
Jégou+, “Searching in One Billion Vectors: Re-rank with Source Coding”, ICASSP 11
Johnson+, “Billion-scale Similarity Search with GPUs”, IEEE TBD 20
Klein and Wolf, “End-to-end Supervised Product Quantization for Image Search and Retrieval”, CVPR 19
Kalantidis and Avrithis, “Locally Optimized Product Quantization for Approximate Nearest Neighbor Search”, CVPR 14
Li+, “Online Variable Coding Length Product Quantization for Fast Nearest Neighbor Search in Mobile Retrieval”, IEEE TMM 17
Martinez+, “Revisiting Additive Quantization”, ECCV 16
Martinez+, “LSQ++: Lower Running Time and Higher Recall in Multi-codebook Quantization”, ECCV 18
Matsui+, “PQTable: Fast Exact Asymmetric Distance Neighbor Search for Product Quantization using Hash Tables”, ICCV 15
Matsui+, “PQk-means: Billion-scale Clustering for Product-quantized Codes”, ACMMM 17
Matsui+, “Reconfigurable Inverted Index”, ACMMM 18
Ning+, “Scalable Image Retrieval by Sparse Product Quantization”, IEEE TMM 17
Norouzi and Fleet, “Cartesian k-means”, CVPR 13
Ozan+, “Competitive Quantization for Approximate Nearest Neighbor Search”, IEEE TKDE 16
Spyromitros-Xious+, “A Comprehensive Study over VLAD and Product Quantization in Large-scale Image Retrieval”, IEEE TMM 14
Yu+, “Product Quantization Network for Fast Image Retrieval”, ECCV 18
Yu+, “Generative Adversarial Product Quantization”, ACMMM 18
Wang+, “Optimized Distances for Binary Code Ranking”, ACMMM 14
Wang+, “Optimized Cartesian k-means”, IEEE TKDE 15
Wang+, “Supervised Quantization for Similarity Search”, CVPR 16
Wang and Zhang, “Composite Quantization”, IEEE TPAMI 19
Wieschollek+, “Efficient Large-scale Approximate Nearest Neighbor Search on the GPU”, CVPR 16
Wu+, “Multiscale Quantization for Fast Similarity Search”, NIPS 17
Wu+, “Quantized Convolutional Neural Networks for Mobile Devices”, CVPR 16
Xia+, “Joint Inverted Indexing”, ICCV 13
Zhang+, “Composite Quantization for Approximate Nearest Neighbor Search”, ICML 14
Zhang+, “Sparse Composite Quantization”, CVPR 15.
Zhang+, “Collaborative Quantization for Crossmodal Similarity Search”, CVPR 16
Zhang+, “Efficient Large-scale Approximate Nearest Neighbor Search on OpenCL FPGA”, CVPR 18
</pre>
</details></section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cpp_api/classlist.html" class="btn btn-neutral float-right" title="Class list" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Facebook, Inc. and its affiliates..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>